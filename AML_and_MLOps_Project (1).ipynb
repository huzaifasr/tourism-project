{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klg2JF-oBblG"
      },
      "source": [
        "# Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0CcOjZ-BblL"
      },
      "source": [
        "## **Business Context**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyT6Koe7BblM"
      },
      "source": [
        "\"Visit with Us,\" a leading travel company, is revolutionizing the tourism industry by leveraging data-driven strategies to optimize operations and customer engagement. While introducing a new package offering, such as the Wellness Tourism Package, the company faces challenges in targeting the right customers efficiently. The manual approach to identifying potential customers is inconsistent, time-consuming, and prone to errors, leading to missed opportunities and suboptimal campaign performance.\n",
        "\n",
        "To address these issues, the company aims to implement a scalable and automated system that integrates customer data, predicts potential buyers, and enhances decision-making for marketing strategies. By utilizing an MLOps pipeline, the company seeks to achieve seamless integration of data preprocessing, model development, deployment, and CI/CD practices for continuous improvement. This system will ensure efficient targeting of customers, timely updates to the predictive model, and adaptation to evolving customer behaviors, ultimately driving growth and customer satisfaction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm6bNQOJBblO"
      },
      "source": [
        "## **Objective**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PYtjk_YBblO"
      },
      "source": [
        "As an MLOps Engineer at \"Visit with Us,\" your responsibility is to design and deploy an MLOps pipeline on GitHub to automate the end-to-end workflow for predicting customer purchases. The primary objective is to build a model that predicts whether a customer will purchase the newly introduced Wellness Tourism Package before contacting them. The pipeline will include data cleaning, preprocessing, transformation, model building, training, evaluation, and deployment, ensuring consistent performance and scalability. By leveraging GitHub Actions for CI/CD integration, the system will enable automated updates, streamline model deployment, and improve operational efficiency. This robust predictive solution will empower policymakers to make data-driven decisions, enhance marketing strategies, and effectively target potential customers, thereby driving customer acquisition and business growth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8C11AzTBblP"
      },
      "source": [
        "## **Data Description**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DQx3pkaBblP"
      },
      "source": [
        "The dataset contains customer and interaction data that serve as key attributes for predicting the likelihood of purchasing the Wellness Tourism Package. The detailed attributes are:\n",
        "\n",
        "**Customer Details**\n",
        "- **CustomerID:** Unique identifier for each customer.\n",
        "- **ProdTaken:** Target variable indicating whether the customer has purchased a package (0: No, 1: Yes).\n",
        "- **Age:** Age of the customer.\n",
        "- **TypeofContact:** The method by which the customer was contacted (Company Invited or Self Inquiry).\n",
        "- **CityTier:** The city category based on development, population, and living standards (Tier 1 > Tier 2 > Tier 3).\n",
        "- **Occupation:** Customer's occupation (e.g., Salaried, Freelancer).\n",
        "- **Gender:** Gender of the customer (Male, Female).\n",
        "- **NumberOfPersonVisiting:** Total number of people accompanying the customer on the trip.\n",
        "- **PreferredPropertyStar:** Preferred hotel rating by the customer.\n",
        "- **MaritalStatus:** Marital status of the customer (Single, Married, Divorced).\n",
        "- **NumberOfTrips:** Average number of trips the customer takes annually.\n",
        "- **Passport:** Whether the customer holds a valid passport (0: No, 1: Yes).\n",
        "- **OwnCar:** Whether the customer owns a car (0: No, 1: Yes).\n",
        "- **NumberOfChildrenVisiting:** Number of children below age 5 accompanying the customer.\n",
        "- **Designation:** Customer's designation in their current organization.\n",
        "- **MonthlyIncome:** Gross monthly income of the customer.\n",
        "\n",
        "**Customer Interaction Data**\n",
        "- **PitchSatisfactionScore:** Score indicating the customer's satisfaction with the sales pitch.\n",
        "- **ProductPitched:** The type of product pitched to the customer.\n",
        "- **NumberOfFollowups:** Total number of follow-ups by the salesperson after the sales pitch.-\n",
        "- **DurationOfPitch:** Duration of the sales pitch delivered to the customer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LbSu_p2jYfe"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "giodc4KknHID"
      },
      "outputs": [],
      "source": [
        "# Create a master folder to keep all files created when executing the below code cells\n",
        "import os\n",
        "os.makedirs(\"tourism_project\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "SUKPoy0EA4jj"
      },
      "outputs": [],
      "source": [
        "# Create a folder for storing the model building files\n",
        "os.makedirs(\"tourism_project/model_building\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DtS3gNDjBbR"
      },
      "source": [
        "## Data Registration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "kNUYcTe-xckI"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"tourism_project/data\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxXiD9ZXxodF"
      },
      "source": [
        "Once the **data** folder created after executing the above cell, please upload the **tourism.csv** in to the folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEG3M03Y9dn7",
        "outputId": "3edbf83f-3f12-464d-bb73-29aee205ec4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tourism_project/model_building/data_register.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tourism_project/model_building/data_register.py\n",
        "from huggingface_hub.utils import RepositoryNotFoundError, HfHubHTTPError\n",
        "from huggingface_hub import HfApi, create_repo\n",
        "import os\n",
        "\n",
        "\n",
        "repo_id = \"huzaifa-sr/tourism-project\"\n",
        "repo_type = \"dataset\"\n",
        "\n",
        "# Initialize API client\n",
        "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
        "\n",
        "# Step 1: Check if the space exists\n",
        "try:\n",
        "    api.repo_info(repo_id=repo_id, repo_type=repo_type)\n",
        "    print(f\"Space '{repo_id}' already exists. Using it.\")\n",
        "except RepositoryNotFoundError:\n",
        "    print(f\"Space '{repo_id}' not found. Creating new space...\")\n",
        "    create_repo(repo_id=repo_id, repo_type=repo_type, private=False)\n",
        "    print(f\"Space '{repo_id}' created.\")\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=\"tourism_project/data\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=repo_type,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh2TjRG5WJ4Z"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "IsV3PNXTN-_2",
        "outputId": "55d52853-2258-49ab-f929-59429ebf6021"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Gender</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Male</th>\n",
              "      <td>2463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Female</th>\n",
              "      <td>1510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Fe Male</th>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "Gender\n",
              "Male       2463\n",
              "Female     1510\n",
              "Fe Male     155\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"Gender\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcb-aj_0PBaq"
      },
      "source": [
        "1. **Imports Necessary Libraries**:\n",
        "\n",
        "2. **Dataset Loading**:\n",
        "   - The script defines a path to a dataset stored on Hugging Face and reads it into a Pandas DataFrame.\n",
        "\n",
        "3. **Data Preparation**:\n",
        "   - The code creates matrices for predictors (features) and the target variable.\n",
        "   - It splits the dataset into training and testing sets, reserving 20% of the data for testing. This is done to evaluate the model's performance later.\n",
        "\n",
        "4. **Saving Prepared Data**:\n",
        "   - After splitting, the script saves the training and testing datasets (features and target) as CSV files.\n",
        "\n",
        "5. **Uploading Files**:\n",
        "   - Finally, it uploads these CSV files back to the Hugging Face Hub, ensuring that they are properly stored in the specified repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC-uQikc_WjJ",
        "outputId": "9ce79f70-dfda-4c6c-d259-b7aeb67c7aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tourism_project/model_building/prep.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tourism_project/model_building/prep.py\n",
        "# for data manipulation\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "# for creating a folder\n",
        "import os\n",
        "# for data preprocessing and pipeline creation\n",
        "from sklearn.model_selection import train_test_split\n",
        "# for converting text data in to numerical representation\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# for hugging face space authentication to upload files\n",
        "from huggingface_hub import login, HfApi\n",
        "import numpy as np\n",
        "\n",
        "# Define constants for the dataset and output paths\n",
        "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
        "DATASET_PATH = \"hf://datasets/huzaifa-sr/tourism-project/tourism.csv\"\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "# Drop columns not required\n",
        "df = df.drop(columns=[\"CustomerID\"])\n",
        "print(\"Dropped CustomerID column\")\n",
        "first_col = df.columns[0]\n",
        "if first_col == \"\" or str(first_col).lower().startswith(\"unnamed\"):\n",
        "    df = df.drop(columns=[first_col])\n",
        "    print(f\"Dropped unnamed index column: {first_col}\")\n",
        "\n",
        "# Ensure target exists\n",
        "TARGET = \"ProdTaken\"\n",
        "if TARGET not in df.columns:\n",
        "    raise KeyError(f\"Expected target column '{TARGET}' not found in dataset columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Fix Gender typos/variants seen in CSV (e.g. 'Fe Male', 'Fe Male ', 'FeMale')\n",
        "if \"Gender\" in df.columns:\n",
        "    df[\"Gender\"] = df[\"Gender\"].astype(str).str.strip().str.lower()\n",
        "    df.loc[df[\"Gender\"].str.contains(r\"fe|fem\", na=False), \"Gender\"] = \"Female\"\n",
        "    df.loc[df[\"Gender\"].str.contains(r\"male\", na=False) & ~df[\"Gender\"].str.contains(r\"fe|fem\", na=False), \"Gender\"] = \"Male\"\n",
        "    df.loc[~df[\"Gender\"].isin([\"Male\", \"Female\"]) , \"Gender\"] = np.nan\n",
        "\n",
        "\n",
        "# Columns to treat as categorical for encoding\n",
        "categorical_cols = [\n",
        "    c for c in [\n",
        "        \"TypeofContact\",\n",
        "        \"Occupation\",\n",
        "        \"Gender\",\n",
        "        \"ProductPitched\",\n",
        "        \"MaritalStatus\",\n",
        "        \"Designation\",\n",
        "    ]\n",
        "    if c in df.columns\n",
        "]\n",
        "\n",
        "# Numeric columns detection (excluding target)\n",
        "numeric_cols = [c for c in df.columns if c not in categorical_cols + [TARGET]]\n",
        "\n",
        "# Convert numeric-like columns to numeric dtype where possible\n",
        "for c in numeric_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# Impute missing values: numeric -> median, categorical -> mode\n",
        "for c in numeric_cols:\n",
        "    if df[c].isna().any():\n",
        "        med = df[c].median()\n",
        "        df[c] = df[c].fillna(med)\n",
        "        print(f\"Imputed numeric column {c} with median={med}\")\n",
        "\n",
        "for c in categorical_cols:\n",
        "    if df[c].isna().any():\n",
        "        mode_val = df[c].mode(dropna=True)\n",
        "        if not mode_val.empty:\n",
        "            mode_val = mode_val[0]\n",
        "            df[c] = df[c].fillna(mode_val)\n",
        "            print(f\"Imputed categorical column {c} with mode='{mode_val}'\")\n",
        "        else:\n",
        "            # if mode cannot be determined, fill with string 'Unknown'\n",
        "            df[c] = df[c].fillna(\"Unknown\")\n",
        "            print(f\"Filled categorical column {c} with 'Unknown' (no mode available)\")\n",
        "\n",
        "# One-hot encode categorical columns using pandas get_dummies (drop first to avoid collinearity)\n",
        "if categorical_cols:\n",
        "    df = pd.get_dummies(df, columns=categorical_cols, prefix_sep=\"__\", drop_first=True)\n",
        "    print(f\"One-hot encoded columns: {categorical_cols}\")\n",
        "\n",
        "# Final check: ensure no missing values remain\n",
        "missing_after = df.isna().sum().sum()\n",
        "print(f\"Total missing values after imputation/encoding: {missing_after}\")\n",
        "\n",
        "# Split into X and y\n",
        "X = df.drop(columns=[TARGET])\n",
        "y = df[TARGET]\n",
        "\n",
        "# Ensure output directory exists\n",
        "out_dir = \"tourism_project/data/prepared\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# Train-test split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if len(y.unique())>1 else None)\n",
        "\n",
        "# Save CSVs\n",
        "Xtrain.to_csv(os.path.join(out_dir, \"Xtrain.csv\"), index=False)\n",
        "Xtest.to_csv(os.path.join(out_dir, \"Xtest.csv\"), index=False)\n",
        "ytrain.to_csv(os.path.join(out_dir, \"ytrain.csv\"), index=False)\n",
        "ytest.to_csv(os.path.join(out_dir, \"ytest.csv\"), index=False)\n",
        "\n",
        "print(f\"Saved prepared files to {out_dir}\")\n",
        "\n",
        "# Optional: upload to Hugging Face dataset repo if HF_TOKEN present in env\n",
        "hf_token = os.getenv(\"HF_TOKEN\")\n",
        "if hf_token:\n",
        "    api = HfApi(token=hf_token)\n",
        "    repo_id = \"huzaifa-sr/tourism-project\"\n",
        "    try:\n",
        "        for filename in [\"Xtrain.csv\",\"Xtest.csv\",\"ytrain.csv\",\"ytest.csv\"]:\n",
        "            path = os.path.join(out_dir, filename)\n",
        "            api.upload_file(path_or_fileobj=path, path_in_repo=filename, repo_id=repo_id, repo_type=\"dataset\")\n",
        "            print(f\"Uploaded {filename} to Hugging Face dataset {repo_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to upload to HF: {e}\")\n",
        "\n",
        "print(\"Data preparation completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZZKnLkLjeM4"
      },
      "source": [
        "## Model Training and Registration with Experimentation Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC0SAaVrwWVq",
        "outputId": "50e3951e-471e-4e42-e583-ade0a222cffc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m718.4/718.4 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install mlflow==3.0.1 pyngrok==7.2.12 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvwsqPBZPpGP"
      },
      "source": [
        "\n",
        "1. **Set Ngrok Authentication**: Authenticates Ngrok using a personal token to enable secure tunneling from local to public network.\n",
        "\n",
        "2. **Launch MLflow UI**: Starts the MLflow Tracking UI as a background process on local port 5000 for experiment visualization and tracking.\n",
        "\n",
        "3. **Create Public Tunnel**: Uses Ngrok to expose the local MLflow UI to the internet, generating a public URL that can be accessed remotely.\n",
        "\n",
        "4. **Display Public URL**: Prints the Ngrok-generated URL, allowing users to open and interact with the MLflow UI in their browser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3KN40uvPsc5",
        "outputId": "b7750fc2-f08d-4777-d50d-cd6a9dcffdf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLflow UI is available at: https://calamitean-yelena-acrogynous.ngrok-free.dev\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import mlflow\n",
        "\n",
        "# Set your auth token here (replace with your actual token)\n",
        "ngrok.set_auth_token(\"33dZWyEHkHqzh0JaqbQ3YrpLvmL_7nj21cUh1dG37JVAj1hH7\")\n",
        "\n",
        "# Start MLflow UI on port 5000\n",
        "process = subprocess.Popen([\"mlflow\", \"ui\", \"--port\", \"5000\"])\n",
        "\n",
        "# Create public tunnel\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "print(\"MLflow UI is available at:\", public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CVFi1x89gkY",
        "outputId": "b0a86ce4-87d1-42e9-978c-f045e1ac6d55"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/05 09:44:23 INFO mlflow.tracking.fluent: Experiment with name 'MLOps_experiment' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='mlflow-artifacts:/719673877582777321', creation_time=1759657463945, experiment_id='719673877582777321', last_update_time=1759657463945, lifecycle_stage='active', name='MLOps_experiment', tags={}>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set the tracking URL for MLflow\n",
        "mlflow.set_tracking_uri(public_url)\n",
        "\n",
        "# Set the name for the experiment\n",
        "mlflow.set_experiment(\"MLOps_experiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBSRfH-UVdIB",
        "outputId": "852eaba0-8837-470c-cdfa-4eb9fa045155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully.\n",
            "Dropped CustomerID column\n",
            "Dropped unnamed index column: Unnamed: 0\n",
            "One-hot encoded columns: ['TypeofContact', 'Occupation', 'Gender', 'ProductPitched', 'MaritalStatus', 'Designation']\n",
            "Total missing values after imputation/encoding: 0\n",
            "Loading prepared data from tourism_project/data/prepared\n",
            "Xtrain shape: (3302, 27), Xtest shape: (826, 27)\n",
            "ytrain distribution:\n",
            "ProdTaken\n",
            "0    0.806784\n",
            "1    0.193216\n",
            "Name: proportion, dtype: float64\n",
            "Numeric features (12): ['Age', 'CityTier', 'DurationOfPitch', 'NumberOfPersonVisiting', 'NumberOfFollowups', 'PreferredPropertyStar', 'NumberOfTrips', 'Passport', 'PitchSatisfactionScore', 'OwnCar']\n",
            "Categorical features (15): ['TypeofContact__Self Enquiry', 'Occupation__Large Business', 'Occupation__Salaried', 'Occupation__Small Business', 'ProductPitched__Deluxe', 'ProductPitched__King', 'ProductPitched__Standard', 'ProductPitched__Super Deluxe', 'MaritalStatus__Married', 'MaritalStatus__Single']\n",
            "scale_pos_weight=4.176 (neg=2664, pos=638)\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [09:50:17] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸƒ View run secretive-donkey-208 at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321/runs/5dc40be57e6d4096ba83418e5dbcc798\n",
            "ğŸ§ª View experiment at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321\n",
            "ğŸƒ View run nebulous-foal-963 at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321/runs/a2a0bb5bbbb843feb8b3f9a639b84262\n",
            "ğŸ§ª View experiment at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321\n",
            "ğŸƒ View run secretive-auk-413 at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321/runs/036d38bea59a47d1a306e62bcace5bf4\n",
            "ğŸ§ª View experiment at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321\n",
            "ğŸƒ View run wistful-goat-714 at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321/runs/ea2fc91b4a9447278d11322486615889\n",
            "ğŸ§ª View experiment at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321\n",
            "ğŸƒ View run luminous-rat-415 at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321/runs/ef31a6e790174829b9bf1688b1a7871d\n",
            "ğŸ§ª View experiment at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321\n",
            "ğŸƒ View run incongruous-rat-81 at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321/runs/86604591938344d3a7b72182a20f3c5a\n",
            "ğŸ§ª View experiment at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321\n",
            "ğŸƒ View run adventurous-pug-440 at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321/runs/28edbb5ecfef43d5b95c5d4b50b5078c\n",
            "ğŸ§ª View experiment at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321\n",
            "ğŸƒ View run useful-kit-902 at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321/runs/d0c467ebd7e94ed4982af8e3c4044370\n",
            "ğŸ§ª View experiment at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321\n",
            "ğŸƒ View run popular-pig-302 at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321/runs/92753ca8d6224f4d9b5bdd132eaa775b\n",
            "ğŸ§ª View experiment at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321\n",
            "Saved best model to tourism_project/model_building/artifacts/xgb_best_model.joblib\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "# for creating a folder\n",
        "import os\n",
        "# for data preprocessing and pipeline creation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# for model training, tuning, and evaluation\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, recall_score\n",
        "# for model serialization\n",
        "import joblib\n",
        "import mlflow\n",
        "import numpy as np\n",
        "\n",
        "# Prefer prepared data (created by prep.py)\n",
        "DATASET_PATH = \"tourism_project/data/tourism.csv\"\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "# Drop columns not required\n",
        "df = df.drop(columns=[\"CustomerID\"])\n",
        "print(\"Dropped CustomerID column\")\n",
        "first_col = df.columns[0]\n",
        "if first_col == \"\" or str(first_col).lower().startswith(\"unnamed\"):\n",
        "    df = df.drop(columns=[first_col])\n",
        "    print(f\"Dropped unnamed index column: {first_col}\")\n",
        "\n",
        "# Ensure target exists\n",
        "TARGET = \"ProdTaken\"\n",
        "if TARGET not in df.columns:\n",
        "    raise KeyError(f\"Expected target column '{TARGET}' not found in dataset columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Fix Gender typos/variants seen in CSV (e.g. 'Fe Male', 'Fe Male ', 'FeMale')\n",
        "if \"Gender\" in df.columns:\n",
        "    df[\"Gender\"] = df[\"Gender\"].astype(str).str.strip().str.lower()\n",
        "    df.loc[df[\"Gender\"].str.contains(r\"fe|fem\", na=False), \"Gender\"] = \"Female\"\n",
        "    df.loc[df[\"Gender\"].str.contains(r\"male\", na=False) & ~df[\"Gender\"].str.contains(r\"fe|fem\", na=False), \"Gender\"] = \"Male\"\n",
        "    df.loc[~df[\"Gender\"].isin([\"Male\", \"Female\"]) , \"Gender\"] = np.nan\n",
        "\n",
        "\n",
        "# Columns to treat as categorical for encoding\n",
        "categorical_cols = [\n",
        "    c for c in [\n",
        "        \"TypeofContact\",\n",
        "        \"Occupation\",\n",
        "        \"Gender\",\n",
        "        \"ProductPitched\",\n",
        "        \"MaritalStatus\",\n",
        "        \"Designation\",\n",
        "    ]\n",
        "    if c in df.columns\n",
        "]\n",
        "\n",
        "# Numeric columns detection (excluding target)\n",
        "numeric_cols = [c for c in df.columns if c not in categorical_cols + [TARGET]]\n",
        "\n",
        "# Convert numeric-like columns to numeric dtype where possible\n",
        "for c in numeric_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# Impute missing values: numeric -> median, categorical -> mode\n",
        "for c in numeric_cols:\n",
        "    if df[c].isna().any():\n",
        "        med = df[c].median()\n",
        "        df[c] = df[c].fillna(med)\n",
        "        print(f\"Imputed numeric column {c} with median={med}\")\n",
        "\n",
        "for c in categorical_cols:\n",
        "    if df[c].isna().any():\n",
        "        mode_val = df[c].mode(dropna=True)\n",
        "        if not mode_val.empty:\n",
        "            mode_val = mode_val[0]\n",
        "            df[c] = df[c].fillna(mode_val)\n",
        "            print(f\"Imputed categorical column {c} with mode='{mode_val}'\")\n",
        "        else:\n",
        "            # if mode cannot be determined, fill with string 'Unknown'\n",
        "            df[c] = df[c].fillna(\"Unknown\")\n",
        "            print(f\"Filled categorical column {c} with 'Unknown' (no mode available)\")\n",
        "\n",
        "# One-hot encode categorical columns using pandas get_dummies (drop first to avoid collinearity)\n",
        "if categorical_cols:\n",
        "    df = pd.get_dummies(df, columns=categorical_cols, prefix_sep=\"__\", drop_first=True)\n",
        "    print(f\"One-hot encoded columns: {categorical_cols}\")\n",
        "\n",
        "# Final check: ensure no missing values remain\n",
        "missing_after = df.isna().sum().sum()\n",
        "print(f\"Total missing values after imputation/encoding: {missing_after}\")\n",
        "\n",
        "# Split into X and y\n",
        "X = df.drop(columns=[TARGET])\n",
        "y = df[TARGET]\n",
        "\n",
        "# Ensure output directory exists\n",
        "out_dir = \"tourism_project/data/prepared\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# Train-test split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if len(y.unique())>1 else None)\n",
        "\n",
        "# Save CSVs\n",
        "Xtrain.to_csv(os.path.join(out_dir, \"Xtrain.csv\"), index=False)\n",
        "Xtest.to_csv(os.path.join(out_dir, \"Xtest.csv\"), index=False)\n",
        "ytrain.to_csv(os.path.join(out_dir, \"ytrain.csv\"), index=False)\n",
        "ytest.to_csv(os.path.join(out_dir, \"ytest.csv\"), index=False)\n",
        "\n",
        "#if os.path.exists(Xtrain_path) and os.path.exists(ytrain_path):\n",
        "print('Loading prepared data from', out_dir)\n",
        "Xtrain_path = os.path.join(out_dir, \"Xtrain.csv\")\n",
        "Xtest_path = os.path.join(out_dir, \"Xtest.csv\")\n",
        "ytrain_path = os.path.join(out_dir, \"ytrain.csv\")\n",
        "ytest_path = os.path.join(out_dir, \"ytest.csv\")\n",
        "\n",
        "Xtrain = pd.read_csv(Xtrain_path)\n",
        "Xtest = pd.read_csv(Xtest_path)\n",
        "ytrain = pd.read_csv(ytrain_path).iloc[:, 0]\n",
        "ytest = pd.read_csv(ytest_path).iloc[:, 0]\n",
        "\n",
        "print(f'Xtrain shape: {Xtrain.shape}, Xtest shape: {Xtest.shape}')\n",
        "print(f'ytrain distribution:\\n{ytrain.value_counts(normalize=True)}')\n",
        "\n",
        "# Determine preprocessing: if Xtrain columns are all numeric we only scale; otherwise scale numeric and one-hot encode categoricals\n",
        "numeric_features = Xtrain.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = Xtrain.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "print(f'Numeric features ({len(numeric_features)}): {numeric_features[:10]}')\n",
        "print(f'Categorical features ({len(categorical_features)}): {categorical_features[:10]}')\n",
        "\n",
        "transformers = []\n",
        "if numeric_features:\n",
        "    transformers.append((StandardScaler(), numeric_features))\n",
        "if categorical_features:\n",
        "    transformers.append((OneHotEncoder(handle_unknown='ignore'), categorical_features))\n",
        "\n",
        "if transformers:\n",
        "    preprocessor = make_column_transformer(*transformers)\n",
        "else:\n",
        "    preprocessor = None\n",
        "\n",
        "# Compute scale_pos_weight for XGBoost to help with class imbalance (neg/pos)\n",
        "neg = (ytrain == 0).sum()\n",
        "pos = (ytrain == 1).sum()\n",
        "scale_pos_weight = neg / pos if pos > 0 else 1.0\n",
        "print(f'scale_pos_weight={scale_pos_weight:.3f} (neg={neg}, pos={pos})')\n",
        "\n",
        "# Define base XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "# set scale_pos_weight\n",
        "xgb_model.set_params(scale_pos_weight=scale_pos_weight)\n",
        "\n",
        "# Build pipeline\n",
        "if preprocessor is not None:\n",
        "    model_pipeline = make_pipeline(preprocessor, xgb_model)\n",
        "else:\n",
        "    model_pipeline = make_pipeline(xgb_model)\n",
        "\n",
        "# Parameter grid (kept small)\n",
        "# Note: when using pipeline, estimator step name becomes 'xgbclassifier' in GridSearchCV param keys if pipeline includes the classifier\n",
        "param_grid = {\n",
        "    'xgbclassifier__n_estimators': [50, 100],\n",
        "    'xgbclassifier__max_depth': [3, 5],\n",
        "    'xgbclassifier__learning_rate': [0.05, 0.1],\n",
        "}\n",
        "\n",
        "with mlflow.start_run():\n",
        "    grid_search = GridSearchCV(model_pipeline, param_grid, cv=3, n_jobs=-1, scoring='f1', verbose=1)\n",
        "    grid_search.fit(Xtrain, ytrain)\n",
        "\n",
        "    results = grid_search.cv_results_\n",
        "    for i in range(len(results['params'])):\n",
        "        param_set = results['params'][i]\n",
        "        mean_score = results['mean_test_score'][i]\n",
        "        std_score = results['std_test_score'][i]\n",
        "        with mlflow.start_run(nested=True):\n",
        "            mlflow.log_params(param_set)\n",
        "            mlflow.log_metric('mean_test_score', mean_score)\n",
        "            mlflow.log_metric('std_test_score', std_score)\n",
        "\n",
        "    mlflow.log_params(grid_search.best_params_)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    classification_threshold = 0.45\n",
        "    try:\n",
        "        y_pred_train_proba = best_model.predict_proba(Xtrain)[:, 1]\n",
        "        y_pred_train = (y_pred_train_proba >= classification_threshold).astype(int)\n",
        "        y_pred_test_proba = best_model.predict_proba(Xtest)[:, 1]\n",
        "        y_pred_test = (y_pred_test_proba >= classification_threshold).astype(int)\n",
        "    except Exception:\n",
        "        y_pred_train = best_model.predict(Xtrain)\n",
        "        y_pred_test = best_model.predict(Xtest)\n",
        "\n",
        "    train_report = classification_report(ytrain, y_pred_train, output_dict=True)\n",
        "    test_report = classification_report(ytest, y_pred_test, output_dict=True)\n",
        "\n",
        "    mlflow.log_metrics({\n",
        "        'train_accuracy': train_report['accuracy'],\n",
        "        'train_precision': train_report.get('1', {}).get('precision', 0),\n",
        "        'train_recall': train_report.get('1', {}).get('recall', 0),\n",
        "        'train_f1-score': train_report.get('1', {}).get('f1-score', 0),\n",
        "        'test_accuracy': test_report['accuracy'],\n",
        "        'test_precision': test_report.get('1', {}).get('precision', 0),\n",
        "        'test_recall': test_report.get('1', {}).get('recall', 0),\n",
        "        'test_f1-score': test_report.get('1', {}).get('f1-score', 0),\n",
        "    })\n",
        "\n",
        "# Save the best model\n",
        "model_dir = 'tourism_project/model_building/artifacts'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "model_path = os.path.join(model_dir, 'xgb_best_model.joblib')\n",
        "joblib.dump(best_model, model_path)\n",
        "print(f'Saved best model to {model_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z26P7bEG2XAJ",
        "outputId": "90364e33-a4cf-441b-de28-59e4d327cefa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully.\n",
            "Dropped CustomerID column\n",
            "Dropped unnamed index column: Unnamed: 0\n",
            "One-hot encoded columns: ['TypeofContact', 'Occupation', 'Gender', 'ProductPitched', 'MaritalStatus', 'Designation']\n",
            "Total missing values after imputation/encoding: 0\n",
            "Loading prepared data from tourism_project/data/prepared\n",
            "Xtrain shape: (3302, 27), Xtest shape: (826, 27)\n",
            "ytrain distribution:\n",
            "ProdTaken\n",
            "0    0.806784\n",
            "1    0.193216\n",
            "Name: proportion, dtype: float64\n",
            "Numeric features (12): ['Age', 'CityTier', 'DurationOfPitch', 'NumberOfPersonVisiting', 'NumberOfFollowups', 'PreferredPropertyStar', 'NumberOfTrips', 'Passport', 'PitchSatisfactionScore', 'OwnCar']\n",
            "Categorical features (15): ['TypeofContact__Self Enquiry', 'Occupation__Large Business', 'Occupation__Salaried', 'Occupation__Small Business', 'ProductPitched__Deluxe', 'ProductPitched__King', 'ProductPitched__Standard', 'ProductPitched__Super Deluxe', 'MaritalStatus__Married', 'MaritalStatus__Single']\n",
            "MLflow UI is available at: https://calamitean-yelena-acrogynous.ngrok-free.dev\n",
            "ğŸƒ View run auspicious-panda-990 at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321/runs/20dfcc25e317489cbdacbfe5d79222c7\n",
            "ğŸ§ª View experiment at: https://calamitean-yelena-acrogynous.ngrok-free.dev/#/experiments/719673877582777321\n",
            "Saved best model to tourism_project/model_building/artifacts/xgb_best_model.joblib\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "# for creating a folder\n",
        "import os\n",
        "# for data preprocessing and pipeline creation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# for model training, tuning, and evaluation\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, recall_score\n",
        "# for model serialization\n",
        "import joblib\n",
        "import mlflow\n",
        "import numpy as np\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Prefer prepared data (created by prep.py)\n",
        "DATASET_PATH = \"tourism_project/data/tourism.csv\"\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "# Drop columns not required\n",
        "df = df.drop(columns=[\"CustomerID\"])\n",
        "print(\"Dropped CustomerID column\")\n",
        "first_col = df.columns[0]\n",
        "if first_col == \"\" or str(first_col).lower().startswith(\"unnamed\"):\n",
        "    df = df.drop(columns=[first_col])\n",
        "    print(f\"Dropped unnamed index column: {first_col}\")\n",
        "\n",
        "# Ensure target exists\n",
        "TARGET = \"ProdTaken\"\n",
        "if TARGET not in df.columns:\n",
        "    raise KeyError(f\"Expected target column '{TARGET}' not found in dataset columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Fix Gender typos/variants seen in CSV (e.g. 'Fe Male', 'Fe Male ', 'FeMale')\n",
        "if \"Gender\" in df.columns:\n",
        "    df[\"Gender\"] = df[\"Gender\"].astype(str).str.strip().str.lower()\n",
        "    df.loc[df[\"Gender\"].str.contains(r\"fe|fem\", na=False), \"Gender\"] = \"Female\"\n",
        "    df.loc[df[\"Gender\"].str.contains(r\"male\", na=False) & ~df[\"Gender\"].str.contains(r\"fe|fem\", na=False), \"Gender\"] = \"Male\"\n",
        "    df.loc[~df[\"Gender\"].isin([\"Male\", \"Female\"]) , \"Gender\"] = np.nan\n",
        "\n",
        "\n",
        "# Columns to treat as categorical for encoding\n",
        "categorical_cols = [\n",
        "    c for c in [\n",
        "        \"TypeofContact\",\n",
        "        \"Occupation\",\n",
        "        \"Gender\",\n",
        "        \"ProductPitched\",\n",
        "        \"MaritalStatus\",\n",
        "        \"Designation\",\n",
        "    ]\n",
        "    if c in df.columns\n",
        "]\n",
        "\n",
        "# Numeric columns detection (excluding target)\n",
        "numeric_cols = [c for c in df.columns if c not in categorical_cols + [TARGET]]\n",
        "\n",
        "# Convert numeric-like columns to numeric dtype where possible\n",
        "for c in numeric_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# Impute missing values: numeric -> median, categorical -> mode\n",
        "for c in numeric_cols:\n",
        "    if df[c].isna().any():\n",
        "        med = df[c].median()\n",
        "        df[c] = df[c].fillna(med)\n",
        "        print(f\"Imputed numeric column {c} with median={med}\")\n",
        "\n",
        "for c in categorical_cols:\n",
        "    if df[c].isna().any():\n",
        "        mode_val = df[c].mode(dropna=True)\n",
        "        if not mode_val.empty:\n",
        "            mode_val = mode_val[0]\n",
        "            df[c] = df[c].fillna(mode_val)\n",
        "            print(f\"Imputed categorical column {c} with mode='{mode_val}'\")\n",
        "        else:\n",
        "            # if mode cannot be determined, fill with string 'Unknown'\n",
        "            df[c] = df[c].fillna(\"Unknown\")\n",
        "            print(f\"Filled categorical column {c} with 'Unknown' (no mode available)\")\n",
        "\n",
        "# One-hot encode categorical columns using pandas get_dummies (drop first to avoid collinearity)\n",
        "if categorical_cols:\n",
        "    df = pd.get_dummies(df, columns=categorical_cols, prefix_sep=\"__\", drop_first=True)\n",
        "    print(f\"One-hot encoded columns: {categorical_cols}\")\n",
        "\n",
        "# Final check: ensure no missing values remain\n",
        "missing_after = df.isna().sum().sum()\n",
        "print(f\"Total missing values after imputation/encoding: {missing_after}\")\n",
        "if missing_after > 0:\n",
        "    print(\"Columns with missing values after imputation/encoding:\")\n",
        "    print(df.isna().sum()[df.isna().sum() > 0])\n",
        "\n",
        "\n",
        "# Split into X and y\n",
        "X = df.drop(columns=[TARGET])\n",
        "y = df[TARGET]\n",
        "\n",
        "# Ensure output directory exists\n",
        "out_dir = \"tourism_project/data/prepared\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# Train-test split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if len(y.unique())>1 else None)\n",
        "\n",
        "# Save CSVs\n",
        "Xtrain.to_csv(os.path.join(out_dir, \"Xtrain.csv\"), index=False)\n",
        "Xtest.to_csv(os.path.join(out_dir, \"Xtest.csv\"), index=False)\n",
        "ytrain.to_csv(os.path.join(out_dir, \"ytrain.csv\"), index=False)\n",
        "ytest.to_csv(os.path.join(out_dir, \"ytest.csv\"), index=False)\n",
        "\n",
        "#if os.path.exists(Xtrain_path) and os.path.exists(ytrain_path):\n",
        "print('Loading prepared data from', out_dir)\n",
        "Xtrain_path = os.path.join(out_dir, \"Xtrain.csv\")\n",
        "Xtest_path = os.path.join(out_dir, \"Xtest.csv\")\n",
        "ytrain_path = os.path.join(out_dir, \"ytrain.csv\")\n",
        "ytest_path = os.path.join(out_dir, \"ytest.csv\")\n",
        "\n",
        "Xtrain = pd.read_csv(Xtrain_path)\n",
        "Xtest = pd.read_csv(Xtest_path)\n",
        "ytrain = pd.read_csv(ytrain_path).iloc[:, 0]\n",
        "ytest = pd.read_csv(ytest_path).iloc[:, 0]\n",
        "\n",
        "print(f'Xtrain shape: {Xtrain.shape}, Xtest shape: {Xtest.shape}')\n",
        "print(f'ytrain distribution:\\n{ytrain.value_counts(normalize=True)}')\n",
        "\n",
        "# Determine preprocessing: if Xtrain columns are all numeric we only scale; otherwise scale numeric and one-hot encode categoricals\n",
        "numeric_features = Xtrain.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = Xtrain.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "print(f'Numeric features ({len(numeric_features)}): {numeric_features[:10]}')\n",
        "print(f'Categorical features ({len(categorical_features)}): {categorical_features[:10]}')\n",
        "\n",
        "# Set the clas weight to handle class imbalance\n",
        "class_weight = ytrain.value_counts()[0] / ytrain.value_counts()[1]\n",
        "class_weight\n",
        "\n",
        "# Define the preprocessing steps\n",
        "preprocessor = make_column_transformer(\n",
        "    (StandardScaler(), numeric_features),\n",
        "    (OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        ")\n",
        "\n",
        "# Define base XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(scale_pos_weight=class_weight, random_state=42)\n",
        "\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'xgbclassifier__n_estimators': [50, 75, 100],\n",
        "    'xgbclassifier__max_depth': [2, 3, 4],\n",
        "    'xgbclassifier__colsample_bytree': [0.4, 0.5, 0.6],\n",
        "    'xgbclassifier__colsample_bylevel': [0.4, 0.5, 0.6],\n",
        "    'xgbclassifier__learning_rate': [0.01, 0.05, 0.1],\n",
        "    'xgbclassifier__reg_lambda': [0.4, 0.5, 0.6],\n",
        "}\n",
        "\n",
        "# Model pipeline\n",
        "model_pipeline = make_pipeline(preprocessor, xgb_model)\n",
        "\n",
        "# Restart ngrok tunnel and set MLflow tracking URI\n",
        "try:\n",
        "    ngrok.kill()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Set your auth token here (replace with your actual token)\n",
        "ngrok.set_auth_token(\"33dZWyEHkHqzh0JaqbQ3YrpLvmL_7nj21cUh1dG37JVAj1hH7\")\n",
        "\n",
        "# Start MLflow UI on port 5000\n",
        "process = subprocess.Popen([\"mlflow\", \"ui\", \"--port\", \"5000\"])\n",
        "\n",
        "# Add a small delay to allow MLflow UI to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Check if the process is still running\n",
        "if process.poll() is not None:\n",
        "    print(\"MLflow UI process failed to start.\")\n",
        "else:\n",
        "    # Create public tunnel\n",
        "    public_url = ngrok.connect(5000).public_url\n",
        "    print(\"MLflow UI is available at:\", public_url)\n",
        "\n",
        "    mlflow.set_tracking_uri(public_url)\n",
        "    mlflow.set_experiment(\"MLOps_experiment\")\n",
        "\n",
        "    with mlflow.start_run():\n",
        "        # Hyperparameter tuning\n",
        "        grid_search = GridSearchCV(model_pipeline, param_grid, cv=5, n_jobs=-1)\n",
        "        grid_search.fit(Xtrain, ytrain)\n",
        "\n",
        "        # Log best parameters\n",
        "        mlflow.log_params(grid_search.best_params_)\n",
        "\n",
        "        # Store and evaluate the best model\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "        classification_threshold = 0.45\n",
        "\n",
        "        y_pred_train_proba = best_model.predict_proba(Xtrain)[:, 1]\n",
        "        y_pred_train = (y_pred_train_proba >= classification_threshold).astype(int)\n",
        "\n",
        "        y_pred_test_proba = best_model.predict_proba(Xtest)[:, 1]\n",
        "        y_pred_test = (y_pred_test_proba >= classification_threshold).astype(int)\n",
        "\n",
        "        train_report = classification_report(ytrain, y_pred_train, output_dict=True)\n",
        "        test_report = classification_report(ytest, y_pred_test, output_dict=True)\n",
        "\n",
        "        mlflow.log_metrics({\n",
        "            \"train_accuracy\": train_report['accuracy'],\n",
        "            \"train_precision\": train_report['1']['precision'],\n",
        "            \"train_recall\": train_report['1']['recall'],\n",
        "            \"train_f1-score\": train_report['1']['f1-score'],\n",
        "            \"test_accuracy\": test_report['accuracy'],\n",
        "            \"test_precision\": test_report['1']['precision'],\n",
        "            \"test_recall\": test_report['1']['recall'],\n",
        "            \"test_f1-score\": test_report['1']['f1-score']\n",
        "        })\n",
        "\n",
        "    # Save the best model\n",
        "    model_dir = 'tourism_project/model_building/artifacts'\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    model_path = os.path.join(model_dir, 'xgb_best_model.joblib')\n",
        "    joblib.dump(best_model, model_path)\n",
        "    print(f'Saved best model to {model_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beWYSPndT1oK",
        "outputId": "40ce11f5-96fa-41cc-9c94-237c98bdae18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tourism_project/model_building/train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tourism_project/model_building/train.py\n",
        "# for data manipulation\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# for model training, tuning, and evaluation\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, recall_score\n",
        "# for model serialization\n",
        "import joblib\n",
        "# for creating a folder\n",
        "import os\n",
        "# for hugging face space authentication to upload files\n",
        "from huggingface_hub import login, HfApi, create_repo\n",
        "from huggingface_hub.utils import RepositoryNotFoundError, HfHubHTTPError\n",
        "import mlflow\n",
        "import numpy as np\n",
        "\n",
        "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
        "mlflow.set_experiment(\"mlops-tourism-project\")\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "\n",
        "Xtrain_path = \"hf://datasets/huzaifa-sr/tourism-project/Xtrain.csv\"\n",
        "Xtest_path = \"hf://datasets/huzaifa-sr/tourism-project/Xtest.csv\"\n",
        "ytrain_path = \"hf://datasets/huzaifa-sr/tourism-project/ytrain.csv\"\n",
        "ytest_path = \"hf://datasets/huzaifa-sr/tourism-project/ytest.csv\"\n",
        "\n",
        "Xtrain = pd.read_csv(Xtrain_path)\n",
        "Xtest = pd.read_csv(Xtest_path)\n",
        "ytrain = pd.read_csv(ytrain_path)\n",
        "ytest = pd.read_csv(ytest_path)\n",
        "\n",
        "Xtrain = pd.read_csv(Xtrain_path)\n",
        "Xtest = pd.read_csv(Xtest_path)\n",
        "ytrain = pd.read_csv(ytrain_path).iloc[:, 0]\n",
        "ytest = pd.read_csv(ytest_path).iloc[:, 0]\n",
        "\n",
        "print(f'Xtrain shape: {Xtrain.shape}, Xtest shape: {Xtest.shape}')\n",
        "print(f'ytrain distribution:\\n{ytrain.value_counts(normalize=True)}')\n",
        "\n",
        "# Determine preprocessing: if Xtrain columns are all numeric we only scale; otherwise scale numeric and one-hot encode categoricals\n",
        "numeric_features = Xtrain.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = Xtrain.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "print(f'Numeric features ({len(numeric_features)}): {numeric_features[:10]}')\n",
        "print(f'Categorical features ({len(categorical_features)}): {categorical_features[:10]}')\n",
        "\n",
        "# Set the clas weight to handle class imbalance\n",
        "class_weight = ytrain.value_counts()[0] / ytrain.value_counts()[1]\n",
        "class_weight\n",
        "\n",
        "# Define the preprocessing steps\n",
        "preprocessor = make_column_transformer(\n",
        "    (StandardScaler(), numeric_features),\n",
        "    (OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        ")\n",
        "\n",
        "# Define base XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(scale_pos_weight=class_weight, random_state=42)\n",
        "\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'xgbclassifier__n_estimators': [50, 75, 100],\n",
        "    'xgbclassifier__max_depth': [2, 3, 4],\n",
        "    'xgbclassifier__colsample_bytree': [0.4, 0.5, 0.6],\n",
        "    'xgbclassifier__colsample_bylevel': [0.4, 0.5, 0.6],\n",
        "    'xgbclassifier__learning_rate': [0.01, 0.05, 0.1],\n",
        "    'xgbclassifier__reg_lambda': [0.4, 0.5, 0.6],\n",
        "}\n",
        "\n",
        "# Model pipeline\n",
        "model_pipeline = make_pipeline(preprocessor, xgb_model)\n",
        "\n",
        "# Start MLflow run\n",
        "with mlflow.start_run():\n",
        "    # Hyperparameter tuning\n",
        "    grid_search = GridSearchCV(model_pipeline, param_grid, cv=5, n_jobs=-1)\n",
        "    grid_search.fit(Xtrain, ytrain)\n",
        "\n",
        "    # Log all parameter combinations and their mean test scores\n",
        "    results = grid_search.cv_results_\n",
        "    for i in range(len(results['params'])):\n",
        "        param_set = results['params'][i]\n",
        "        mean_score = results['mean_test_score'][i]\n",
        "        std_score = results['std_test_score'][i]\n",
        "\n",
        "        # Log each combination as a separate MLflow run\n",
        "        with mlflow.start_run(nested=True):\n",
        "            mlflow.log_params(param_set)\n",
        "            mlflow.log_metric(\"mean_test_score\", mean_score)\n",
        "            mlflow.log_metric(\"std_test_score\", std_score)\n",
        "\n",
        "    # Log best parameters separately in main run\n",
        "    mlflow.log_params(grid_search.best_params_)\n",
        "\n",
        "    # Store and evaluate the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    classification_threshold = 0.45\n",
        "\n",
        "    y_pred_train_proba = best_model.predict_proba(Xtrain)[:, 1]\n",
        "    y_pred_train = (y_pred_train_proba >= classification_threshold).astype(int)\n",
        "\n",
        "    y_pred_test_proba = best_model.predict_proba(Xtest)[:, 1]\n",
        "    y_pred_test = (y_pred_test_proba >= classification_threshold).astype(int)\n",
        "\n",
        "    train_report = classification_report(ytrain, y_pred_train, output_dict=True)\n",
        "    test_report = classification_report(ytest, y_pred_test, output_dict=True)\n",
        "\n",
        "    # Log the metrics for the best model\n",
        "    mlflow.log_metrics({\n",
        "        \"train_accuracy\": train_report['accuracy'],\n",
        "        \"train_precision\": train_report['1']['precision'],\n",
        "        \"train_recall\": train_report['1']['recall'],\n",
        "        \"train_f1-score\": train_report['1']['f1-score'],\n",
        "        \"test_accuracy\": test_report['accuracy'],\n",
        "        \"test_precision\": test_report['1']['precision'],\n",
        "        \"test_recall\": test_report['1']['recall'],\n",
        "        \"test_f1-score\": test_report['1']['f1-score']\n",
        "    })\n",
        "\n",
        "    # Save the model locally\n",
        "    model_path = \"tourism_model_v1.joblib\"\n",
        "    joblib.dump(best_model, model_path)\n",
        "\n",
        "    # Log the model artifact\n",
        "    mlflow.log_artifact(model_path, artifact_path=\"model\")\n",
        "    print(f\"Model saved as artifact at: {model_path}\")\n",
        "\n",
        "    # Upload to Hugging Face\n",
        "    repo_id = \"huzaifa-sr/tourism-project\"\n",
        "    repo_type = \"model\"\n",
        "\n",
        "    # Step 1: Check if the space exists\n",
        "    try:\n",
        "        api.repo_info(repo_id=repo_id, repo_type=repo_type)\n",
        "        print(f\"Space '{repo_id}' already exists. Using it.\")\n",
        "    except RepositoryNotFoundError:\n",
        "        print(f\"Space '{repo_id}' not found. Creating new space...\")\n",
        "        create_repo(repo_id=repo_id, repo_type=repo_type, private=False)\n",
        "        print(f\"Space '{repo_id}' created.\")\n",
        "\n",
        "    # create_repo(\"churn-model\", repo_type=\"model\", private=False)\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=\"tourism_model_v1.joblib\",\n",
        "        path_in_repo=\"tourism_model_v1.joblib\",\n",
        "        repo_id=repo_id,\n",
        "        repo_type=repo_type,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0McYCZzkji5I"
      },
      "source": [
        "# Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QrY2v77vbEZ"
      },
      "source": [
        "## Dockerfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0-AMAI72CR-T"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"tourism_project/deployment\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTicTDnPCVZr",
        "outputId": "cec72415-ecc7-4c03-e5e1-b7e3f48b603e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing tourism_project/deployment/Dockerfile\n"
          ]
        }
      ],
      "source": [
        "%%writefile tourism_project/deployment/Dockerfile\n",
        "# Use a minimal base image with Python 3.9 installed\n",
        "FROM python:3.9\n",
        "\n",
        "# Set the working directory inside the container to /app\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy all files from the current directory on the host to the container's /app directory\n",
        "COPY . .\n",
        "\n",
        "# Install Python dependencies listed in requirements.txt\n",
        "RUN pip3 install -r requirements.txt\n",
        "\n",
        "RUN useradd -m -u 1000 user\n",
        "USER user\n",
        "ENV HOME=/home/user \\\n",
        "\tPATH=/home/user/.local/bin:$PATH\n",
        "\n",
        "WORKDIR $HOME/app\n",
        "\n",
        "COPY --chown=user . $HOME/app\n",
        "\n",
        "# Define the command to run the Streamlit app on port \"8501\" and make it accessible externally\n",
        "CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\", \"--server.enableXsrfProtection=false\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCvrklrBwNvJ"
      },
      "source": [
        "## Streamlit App"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXWe6ObRjP6-"
      },
      "source": [
        "Please ensure that the web app script is named `app.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQDkqYzRD1FV",
        "outputId": "97bcd066-6e52-4b8b-c734-e27a298939c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing tourism_project/deployment/app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tourism_project/deployment/app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "st.set_page_config(page_title='Tourism Purchase Predictor', layout='wide')\n",
        "\n",
        "# Paths\n",
        "MODEL_PATH = 'tourism_project/model_building/artifacts/xgb_best_model.joblib'\n",
        "PREPARED_X_PATH = 'tourism_project/data/prepared/Xtrain.csv'\n",
        "RAW_CSV_CANDIDATES = ['tourism_project/data/tourism.csv', 'tourism.csv']\n",
        "\n",
        "# Load model if available\n",
        "model = None\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    try:\n",
        "        model = joblib.load(MODEL_PATH)\n",
        "        st.info(f'Loaded model from {MODEL_PATH}')\n",
        "    except Exception as e:\n",
        "        st.error(f'Failed to load model at {MODEL_PATH}: {e}')\n",
        "else:\n",
        "    st.warning(f'No trained model found at {MODEL_PATH}. Please run training first.')\n",
        "\n",
        "# Load raw CSV (for populating select options)\n",
        "raw_df = None\n",
        "for p in RAW_CSV_CANDIDATES:\n",
        "    if os.path.exists(p):\n",
        "        raw_df = pd.read_csv(p)\n",
        "        break\n",
        "\n",
        "if raw_df is None:\n",
        "    st.error('Could not find tourism.csv in repo. Place it at repo root or tourism_project/data.')\n",
        "    st.stop()\n",
        "\n",
        "# Quick cleanup matching prep.py behavior\n",
        "# Drop unnamed index column if present\n",
        "first_col = raw_df.columns[0]\n",
        "if first_col == '' or str(first_col).lower().startswith('unnamed'):\n",
        "    raw_df = raw_df.drop(columns=[first_col])\n",
        "\n",
        "# Drop CustomerID if present\n",
        "if 'CustomerID' in raw_df.columns:\n",
        "    raw_df = raw_df.drop(columns=['CustomerID'])\n",
        "\n",
        "# Ensure target exists\n",
        "TARGET = 'ProdTaken'\n",
        "if TARGET not in raw_df.columns:\n",
        "    st.error(f\"Expected target column '{TARGET}' not found in dataset.\")\n",
        "    st.stop()\n",
        "\n",
        "# Candidate input fields (based on dataset)\n",
        "numeric_inputs = [\n",
        "    'Age', 'DurationOfPitch', 'NumberOfPersonVisiting', 'NumberOfFollowups',\n",
        "    'PreferredPropertyStar', 'NumberOfTrips', 'PitchSatisfactionScore',\n",
        "    'NumberOfChildrenVisiting', 'MonthlyIncome'\n",
        "]\n",
        "categorical_inputs = [\n",
        "    'TypeofContact', 'CityTier', 'Occupation', 'Gender', 'ProductPitched',\n",
        "    'MaritalStatus', 'Designation', 'Passport', 'OwnCar'\n",
        "]\n",
        "# Make sure fields exist in raw_df\n",
        "numeric_inputs = [c for c in numeric_inputs if c in raw_df.columns]\n",
        "categorical_inputs = [c for c in categorical_inputs if c in raw_df.columns]\n",
        "\n",
        "st.title('Tourism Package Purchase Predictor')\n",
        "st.write('Enter customer and interaction details below and click Predict.')\n",
        "\n",
        "with st.form('input_form'):\n",
        "    cols = st.columns(3)\n",
        "\n",
        "    inputs = {}\n",
        "    # Numeric inputs\n",
        "    for i, col in enumerate(numeric_inputs):\n",
        "        c = cols[i % 3]\n",
        "        mean = float(raw_df[col].dropna().mean()) if not raw_df[col].dropna().empty else 0.0\n",
        "        inputs[col] = c.number_input(col, value=mean)\n",
        "\n",
        "    # Categorical / binary inputs\n",
        "    for col in categorical_inputs:\n",
        "        if col in ['Passport', 'OwnCar']:\n",
        "            # binary\n",
        "            unique_vals = sorted(raw_df[col].dropna().unique().tolist())\n",
        "            default = 1 if 1 in unique_vals else (0 if 0 in unique_vals else unique_vals[0])\n",
        "            inputs[col] = st.selectbox(col, options=unique_vals, index=0)\n",
        "        elif col == 'CityTier':\n",
        "            # CityTier numeric but categorical-like\n",
        "            options = sorted(raw_df[col].dropna().unique().tolist())\n",
        "            inputs[col] = st.selectbox(col, options=options, index=0)\n",
        "        else:\n",
        "            options = sorted(raw_df[col].dropna().astype(str).unique().tolist())\n",
        "            inputs[col] = st.selectbox(col, options=options, index=0)\n",
        "\n",
        "    submitted = st.form_submit_button('Predict')\n",
        "\n",
        "# Normalize Gender similar to prep.py\n",
        "if 'Gender' in inputs:\n",
        "    g = str(inputs['Gender']).strip().lower()\n",
        "    if any(x in g for x in ['fe', 'fem']):\n",
        "        inputs['Gender'] = 'Female'\n",
        "    elif 'male' in g:\n",
        "        inputs['Gender'] = 'Male'\n",
        "    else:\n",
        "        inputs['Gender'] = inputs['Gender']\n",
        "\n",
        "# Build single-row DataFrame from inputs using the original raw columns order\n",
        "input_df = pd.DataFrame([inputs])\n",
        "\n",
        "# Apply same one-hot encoding used in prep.py (pandas.get_dummies with drop_first and prefix_sep='__')\n",
        "cat_for_dummies = [c for c in categorical_inputs if c in input_df.columns and input_df[c].dtype == object or isinstance(input_df[c].iloc[0], str)]\n",
        "if cat_for_dummies:\n",
        "    input_dummies = pd.get_dummies(input_df, columns=cat_for_dummies, prefix_sep='__', drop_first=True)\n",
        "else:\n",
        "    input_dummies = input_df.copy()\n",
        "\n",
        "# If prepared Xtrain exists, align columns to it (this ensures dummies match training features)\n",
        "if os.path.exists(PREPARED_X_PATH):\n",
        "    prepared_cols = pd.read_csv(PREPARED_X_PATH, nrows=0).columns.tolist()\n",
        "    # Reindex to prepared columns, filling missing with 0\n",
        "    input_prepared = input_dummies.reindex(columns=prepared_cols, fill_value=0)\n",
        "else:\n",
        "    # If no prepared Xtrain file, pass input_dummies as-is and hope pipeline handles raw columns\n",
        "    input_prepared = input_dummies.copy()\n",
        "\n",
        "st.subheader('Prepared input (what will be fed to the model)')\n",
        "st.dataframe(input_prepared.transpose())\n",
        "\n",
        "if submitted:\n",
        "    if model is None:\n",
        "        st.error('No model loaded; cannot predict. Train model and save to the expected path.')\n",
        "    else:\n",
        "        try:\n",
        "            # Ensure columns order matches model training\n",
        "            # If model is a pipeline expecting a full feature set, the input_prepared must match that\n",
        "            X_in = input_prepared\n",
        "            # Some sklearn pipelines expect numpy arrays - convert accordingly\n",
        "            if hasattr(model, 'predict_proba'):\n",
        "                proba = model.predict_proba(X_in)[:, 1][0]\n",
        "                pred = int(proba >= 0.5)\n",
        "            else:\n",
        "                pred = int(model.predict(X_in)[0])\n",
        "                proba = None\n",
        "\n",
        "            st.markdown('## Prediction')\n",
        "            if proba is not None:\n",
        "                st.metric('Probability of purchase (ProdTaken=1)', f'{proba:.3f}')\n",
        "                st.write('Predicted class:', 'Will Purchase (1)' if pred == 1 else 'Will Not Purchase (0)')\n",
        "            else:\n",
        "                st.write('Predicted class:', 'Will Purchase (1)' if pred == 1 else 'Will Not Purchase (0)')\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f'Prediction failed: {e}')\n",
        "\n",
        "# Optionally show a few sample rows from dataset and model prediction\n",
        "st.sidebar.markdown('## Dataset samples')\n",
        "if st.sidebar.button('Show 5 random samples'):\n",
        "    st.sidebar.dataframe(raw_df.sample(5))\n",
        "\n",
        "st.sidebar.markdown('Model & data paths')\n",
        "st.sidebar.text(MODEL_PATH)\n",
        "st.sidebar.text(PREPARED_X_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07cYzWcIwTL-"
      },
      "source": [
        "## Dependency Handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEgfHL64jU7o"
      },
      "source": [
        "Please ensure that the dependency handling file is named `requirements.txt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvdmy7Wd9lda",
        "outputId": "7bd98a52-0da5-4a4e-e437-b5ce5c70fa4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing tourism_project/deployment/requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile tourism_project/deployment/requirements.txt\n",
        "pandas==2.2.2\n",
        "huggingface_hub==0.32.6\n",
        "streamlit==1.43.2\n",
        "joblib==1.5.1\n",
        "scikit-learn==1.6.0\n",
        "xgboost==2.1.4\n",
        "mlflow==3.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4ynzpKNwWS_"
      },
      "source": [
        "# Hosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "7p5sBvTg9nCW"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"tourism_project/hosting\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiiqJm_gFvLq",
        "outputId": "95b47a1f-e2ba-4676-a8ed-6d865c2f078d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tourism_project/hosting/hosting.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tourism_project/hosting/hosting.py\n",
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "\n",
        "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
        "api.upload_folder(\n",
        "    folder_path=\"tourism_project/deployment\",     # the local folder containing your files\n",
        "    repo_id=\"huzaifa-sr/tourism-project\",          # the target repo\n",
        "    repo_type=\"space\",                      # dataset, model, or space\n",
        "    path_in_repo=\"\",                          # optional: subfolder path inside the repo\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuCgAW2hktli"
      },
      "source": [
        "# MLOps Pipeline with Github Actions Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5BZr5i8PKVN"
      },
      "source": [
        "**Note:**\n",
        "\n",
        "1. Before running the file below, make sure to add the HF_TOKEN to your GitHub secrets to enable authentication between GitHub and Hugging Face.\n",
        "2. The below code is for a sample YAML file that can be updated as required to meet the requirements of this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J029tYPq4Rmq"
      },
      "source": [
        "```\n",
        "name: Tourism pipeline\n",
        "\n",
        "on:\n",
        "  push:\n",
        "    branches:\n",
        "      - main  # Automatically triggers on push to the main branch\n",
        "\n",
        "jobs:\n",
        "\n",
        "  register-dataset:\n",
        "    runs-on: ubuntu-latest\n",
        "    steps:\n",
        "      - uses: actions/checkout@v3\n",
        "      - name: Install Dependencies\n",
        "        run: pip install -r tourism_project/requirements.txt\n",
        "      - name: Upload Dataset to Hugging Face Hub\n",
        "        env:\n",
        "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
        "        run: python tourism_project/model_building/data_register.py\n",
        "\n",
        "  data-prep:\n",
        "    needs: register-dataset\n",
        "    runs-on: ubuntu-latest\n",
        "    steps:\n",
        "      - uses: actions/checkout@v3\n",
        "      - name: Install Dependencies\n",
        "        run: pip install -r tourism_project/requirements.txt\n",
        "      - name: Run Data Preparation\n",
        "        env:\n",
        "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
        "        run: python tourism_project/model_building/prep.py\n",
        "\n",
        "\n",
        "  model-traning:\n",
        "    needs: data-prep\n",
        "    runs-on: ubuntu-latest\n",
        "    steps:\n",
        "      - uses: actions/checkout@v3\n",
        "      - name: Install Dependencies\n",
        "        run: pip install -r tourism_project/requirements.txt\n",
        "      - name: Start MLflow Server\n",
        "        run: |\n",
        "          nohup mlflow ui --host 0.0.0.0 --port 5000 &  # Run MLflow UI in the background\n",
        "          sleep 5  # Wait for a moment to let the server starts\n",
        "      - name: Model Building\n",
        "        env:\n",
        "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
        "        run: python tourism_project/model_building/train.py\n",
        "\n",
        "\n",
        "  deploy-hosting:\n",
        "    runs-on: ubuntu-latest\n",
        "    needs: [model-traning,data-prep,register-dataset]\n",
        "    steps:\n",
        "      - uses: actions/checkout@v3\n",
        "      - name: Install Dependencies\n",
        "        run: pip install -r tourism_project/requirements.txt\n",
        "      - name: Push files to Frontend Hugging Face Space\n",
        "        env:\n",
        "          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n",
        "        run: python tourism_project/hosting/hosting.py\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9fgZ_Mq3zzp"
      },
      "source": [
        "**Note:** To use this YAML file for our use case, we need to\n",
        "\n",
        "1. Go to the GitHub repository for the project\n",
        "2. Create a folder named ***.github/workflows/***\n",
        "3. In the above folder, create a file named ***pipeline.yml***\n",
        "4. Copy and paste the above content for the YAML file into the ***pipeline.yml*** file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvEUJ-t5kdxH"
      },
      "source": [
        "## Requirements file for the Github Actions Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfqWcLRm-dga",
        "outputId": "73c2d53b-b613-4653-e11b-c096ff6ac3a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing tourism_project/requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile tourism_project/requirements.txt\n",
        "huggingface_hub==0.32.6\n",
        "datasets==3.6.0\n",
        "pandas==2.2.2\n",
        "scikit-learn==1.6.0\n",
        "xgboost==2.1.4\n",
        "mlflow==3.0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA6mP-Ebkm3O"
      },
      "source": [
        "## Github Authentication and Push Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T84Ei-g9Z2uw"
      },
      "source": [
        "* Before moving forward, we need to generate a secret token to push files directly from Colab to the GitHub repository.\n",
        "* Please follow the below instructions to create the GitHub token:\n",
        "    - Open your GitHub profile.\n",
        "    - Click on ***Settings***.\n",
        "    - Go to ***Developer Settings***.\n",
        "    - Expand the ***Personal access tokens*** section and select ***Tokens (classic)***.\n",
        "    - Click ***Generate new token***, then choose ***Generate new token (classic)***.\n",
        "    - Add a note and select all required scopes.\n",
        "    - Click ***Generate token***.\n",
        "    - Copy the generated token and store it safely in a notepad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPDx4gqGh7cO",
        "outputId": "ba523d7e-dee0-4abf-f4ad-a5c84c17112b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.15).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "fatal: destination path 'tourism-project' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Install Git\n",
        "!apt-get install git\n",
        "\n",
        "# Set your Git identity (replace with your details)\n",
        "!git config --global user.email \"huzaifa.sr@gmail.com\"\n",
        "!git config --global user.name \"huzaifasr\"\n",
        "\n",
        "# Clone your GitHub repository\n",
        "!git clone https://github.com/huzaifasr/tourism-project.git\n",
        "\n",
        "# Move your folder to the repository directory\n",
        "!mv /content/tourism_project/ /content/tourism-project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "awvnua3RRN9P"
      },
      "outputs": [],
      "source": [
        "!git add data/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "Wk2jiMo1RX8L"
      },
      "outputs": [],
      "source": [
        "!git add deployment/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "9dPpHYyDRbHB"
      },
      "outputs": [],
      "source": [
        "!git add hosting/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "sHGvodzxRa8F"
      },
      "outputs": [],
      "source": [
        "!git add model_building/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "hsVfZxSCRazy"
      },
      "outputs": [],
      "source": [
        "!git add deployment/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXGlxVfZRyyu",
        "outputId": "63324b4f-578b-46ae-885c-c1b60f65a2ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch main\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31mtourism-project/\u001b[m\n",
            "\t\u001b[31mtourism_project/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n"
          ]
        }
      ],
      "source": [
        "# Commit the changes\n",
        "!git commit -m \"Add tourism_project files1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuUahCwVigon",
        "outputId": "7b249ba1-0ffe-45d5-acae-a8e76ee3d4fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "error: 'tourism-project/' does not have a commit checked out\n",
            "fatal: adding files failed\n",
            "On branch main\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31mtourism-project/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n",
            "Enumerating objects: 22, done.\n",
            "Counting objects: 100% (22/22), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (19/19), done.\n",
            "Writing objects: 100% (21/21), 188.45 KiB | 3.43 MiB/s, done.\n",
            "Total 21 (delta 0), reused 0 (delta 0), pack-reused 0\n",
            "To https://github.com/huzaifasr/tourism-project.git\n",
            "   046c204..e1d9a95  main -> main\n"
          ]
        }
      ],
      "source": [
        "# Change directory to the cloned repository\n",
        "# %cd tourism-project/ # Removed this line as it's causing an error and the directory should already be correct\n",
        "\n",
        "# Add the new folder to Git\n",
        "!git add .\n",
        "\n",
        "# Commit the changes\n",
        "!git commit -m \"Add tourism_project files\"\n",
        "\n",
        "# Push to GitHub using the token in the URL (replace YOUR_GITHUB_PERSONAL_ACCESS_TOKEN)\n",
        "!git push https://huzaifasr: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@github.com/huzaifasr/tourism-project.git main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-i8Jdyz-_L1"
      },
      "source": [
        "# Output Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTK8Bpda_UHg"
      },
      "source": [
        "- GitHub (link to repository, screenshot of folder structure and executed workflow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qzzesaG_Xw8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KDN31V2_YSr"
      },
      "source": [
        "- Streamlit on Hugging Face (link to HF space, screenshot of Streamlit app)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuIUdj3b_ZYV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN8j9-3nW8G9"
      },
      "source": [
        "<font size=6 color=\"navyblue\">Power Ahead!</font>\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "klg2JF-oBblG",
        "m0CcOjZ-BblL",
        "zm6bNQOJBblO",
        "z8C11AzTBblP",
        "0LbSu_p2jYfe",
        "9DtS3gNDjBbR",
        "hh2TjRG5WJ4Z",
        "eZZKnLkLjeM4",
        "0McYCZzkji5I",
        "9QrY2v77vbEZ",
        "LCvrklrBwNvJ",
        "07cYzWcIwTL-",
        "V4ynzpKNwWS_",
        "PuCgAW2hktli",
        "PvEUJ-t5kdxH",
        "BA6mP-Ebkm3O",
        "v-i8Jdyz-_L1"
      ],
      "provenance": []
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
